import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error
from statsmodels.tsa.arima.model import ARIMA

# --- 1. é é¢è¨­å®š / Page Configuration ---
st.set_page_config(page_title="SCM AI Multi-Model Study", layout="wide")


# --- 2. è³‡æ–™è¼‰å…¥ / Data Collection ---
@st.cache_data
def load_data():
    try:
        # è®€å–æ­·å²éŠ·å”®æ•¸æ“š (Historical sales / Data collection)
        df = pd.read_csv('meat_consumption_worldwide.csv')
        return df
    except Exception as e:
        st.error(f"æ‰¾ä¸åˆ°è³‡æ–™ï¼è«‹æª¢æŸ¥ CSV æª”æ¡ˆã€‚(Data not found!): {e}")
        return None


# --- 3. æ ¸å¿ƒé‹ç®—é‚è¼¯ / ML & Statistical Algorithms ---
def run_comparison(values, test_size=5):
    """
    åŸ·è¡Œæµç¨‹åœ–ä¸­çš„æ‰€æœ‰é æ¸¬æ¼”ç®—æ³•ã€‚
    Running all prediction algorithms defined in the flowchart.
    """
    look_back = 3
    y_true = values[-test_size:]
    train_data = values[:-test_size]

    # --- æ¨¡å‹ A: å‚³çµ± SMA (Baseline) ---
    y_pred_sma = [np.mean(values[-(test_size + look_back + i): -(test_size + i)]) for i in range(test_size, 0, -1)]

    # ç‰¹å¾µå·¥ç¨‹ (ç”¨æ–¼ RF èˆ‡ LR)
    X_train, y_train = [], []
    for i in range(len(train_data) - look_back):
        X_train.append(train_data[i: i + look_back])
        y_train.append(train_data[i + look_back])
    X_test = [values[-(test_size + look_back - i): -(test_size - i)] for i in range(test_size)]

    # --- æ¨¡å‹ B: Random Forest (ML Algorithms) ---
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)

    # --- æ¨¡å‹ C: Linear Regression (Statistical Control) ---
    lr = LinearRegression()
    lr.fit(X_train, y_train)
    y_pred_lr = lr.predict(X_test)

    # --- æ¨¡å‹ D: ARIMA (Time Series Model) ---
    try:
        history = list(train_data)
        y_pred_arima = []
        for i in range(test_size):
            # å»ºç«‹ ARIMA(1,1,0) æ¨¡å‹
            model = ARIMA(history, order=(1, 1, 0))
            model_fit = model.fit()
            y_pred_arima.append(model_fit.forecast()[0])
            history.append(y_true[i])  # æ»¾å‹•æ›´æ–°æ­·å²è³‡æ–™
    except:
        y_pred_arima = y_pred_sma

    return {
        "Actual": y_true,
        "SMA": np.array(y_pred_sma),
        "Random Forest": np.array(y_pred_rf),
        "Linear Regression": np.array(y_pred_lr),
        "ARIMA": np.array(y_pred_arima)
    }


# --- 4. ä»‹é¢å‘ˆç¾ / UI Dashboard ---
df_all = load_data()

if df_all is not None:
    st.sidebar.header("âš™ï¸ å¯¦é©—è¨­ç½® (Experiment Setup)")
    dataset_option = st.sidebar.selectbox(
        "é¸æ“‡è³‡æ–™é›† (Case Study Selection)",
        ["USA - BEEF (ç©©å®š/Stable)", "CHN - PIG (é«˜æ³¢å‹•/Volatile)", "EU28 - POULTRY (è¶¨å‹¢/Trend)"]
    )

    mapping = {
        "USA - BEEF (ç©©å®š/Stable)": ("USA", "BEEF"),
        "CHN - PIG (é«˜æ³¢å‹•/Volatile)": ("CHN", "PIG"),
        "EU28 - POULTRY (è¶¨å‹¢/Trend)": ("EU28", "POULTRY")
    }
    loc, sub = mapping[dataset_option]
    df_target = df_all[
        (df_all['LOCATION'] == loc) & (df_all['SUBJECT'] == sub) & (df_all['MEASURE'] == 'THND_TONNE')].sort_values(
        'TIME')
    df_target['DATE'] = df_target['TIME'].apply(lambda x: f"{int(x)}")
    raw_values = df_target['Value'].values

    st.title("ğŸ›¡ï¸ ä¾›æ‡‰éˆéœ€æ±‚é æ¸¬å°ç…§ç ”ç©¶ (SCM Forecasting Analysis)")

    # ä½¿ç”¨æŒ‰éˆ•åŸ·è¡Œåˆ†æä¸¦å„²å­˜ç‹€æ…‹ï¼Œé¿å…é¸å–®åˆ‡æ›æ™‚è³‡æ–™éºå¤±
    if st.button("ğŸš€ åŸ·è¡Œå¤šæ¨¡å‹å…¨è‡ªå‹•åˆ†æ (Execute All Models)"):
        st.session_state['scm_results'] = run_comparison(raw_values)
        st.session_state['scm_dates'] = df_target['DATE'].values[-5:]

    # æª¢æŸ¥æ˜¯å¦æœ‰é‹ç®—çµæœ
    if 'scm_results' in st.session_state:
        results = st.session_state['scm_results']
        test_dates = st.session_state['scm_dates']
        y_true = results["Actual"]

        tab1, tab2, tab3 = st.tabs([
            "ğŸ“ˆ é æ¸¬åˆ†æ (Predictive Analytics)",
            "ğŸ§ª ä¸ç¢ºå®šæ€§æ¨¡æ“¬ (Uncertainty Simulation)",
            "ğŸ§  ä¸­è‹±å°ç…§èˆ‡çµè«– (Glossary & Conclusion)"
        ])

        # --- Tab 1: å¯è¦–åŒ–å°æ¯” ---
        with tab1:
            st.subheader("æ¨¡å‹é æ¸¬çµæœå¯è¦–åŒ– (Forecasting Visibility)")
            fig = go.Figure()
            fig.add_trace(go.Scatter(x=test_dates, y=y_true, name="å¯¦éš›å€¼ (Actual)", line=dict(color='black', width=4)))
            for m in ["SMA", "Random Forest", "Linear Regression", "ARIMA"]:
                fig.add_trace(go.Scatter(x=test_dates, y=results[m], name=m))

            fig.update_layout(xaxis_title="å¹´ä»½ (Year)", yaxis_title="éœ€æ±‚é‡ (Demand)", template="plotly_white")
            st.plotly_chart(fig, use_container_width=True)

            # é¡¯ç¤º MAPE ç¸¾æ•ˆ
            st.subheader("ğŸ¯ æº–ç¢ºç‡æŒ‡æ¨™å°ç…§ (Forecast Accuracy Metrics)")
            cols = st.columns(4)
            for i, m in enumerate(["SMA", "Random Forest", "Linear Regression", "ARIMA"]):
                mape = mean_absolute_percentage_error(y_true, results[m]) * 100
                cols[i].metric(m, f"{mape:.2f}%")

        # --- Tab 2: æ®˜å·®èˆ‡ä¸ç¢ºå®šæ€§ (æ”¯æ´æ‰€æœ‰æ¨¡å‹åˆ‡æ›) ---
        with tab2:
            st.subheader("ğŸ§ª æ¨¡å‹æ®˜å·®èˆ‡ä¸ç¢ºå®šæ€§åˆ†æ (Residuals Comparison)")
            st.write("æ‚¨å¯ä»¥åˆ‡æ›ä¸‹æ–¹é¸å–®ï¼Œæ¯”è¼ƒä¸åŒæ¨¡å‹åœ¨æ“¾å‹•å ´æ™¯ä¸‹çš„ç©©å®šæ€§ï¼š")

            # é€™è£¡åŒ…å«æ‰€æœ‰ç”¨åˆ°çš„æ¨¡å‹ (All models included)
            selected_model = st.selectbox(
                "é¸æ“‡åˆ†æå°è±¡ (Select Model)",
                ["Random Forest", "ARIMA", "Linear Regression", "SMA"]
            )

            res_vals = y_true - results[selected_model]
            colors = ['#87CEEB' if r >= 0 else '#FF7F7F' for r in res_vals]

            fig_res = go.Figure()
            fig_res.add_trace(go.Bar(x=test_dates, y=res_vals, marker_color=colors, name=f"{selected_model} Residuals"))
            fig_res.update_layout(
                title=f"<b>{selected_model} æ®˜å·®åˆ†ä½ˆ (Residuals Analysis)</b>",
                xaxis_title="å¹´ä»½", yaxis_title="é æ¸¬èª¤å·® (Error)", template="plotly_white"
            )
            st.plotly_chart(fig_res, use_container_width=True)

            # ä¸­è‹±å°ç…§è§£é‡‹
            c1, c2 = st.columns(2)
            with c1:
                st.markdown(f"""
                **åˆ†ææ¨¡å‹ (Model):** {selected_model}
                * **Positive (æ­£å€¼):** Under-forecast (å¯¦éš› > é æ¸¬) â†’ **ç¼ºè²¨é¢¨éšª**
                * **Negative (è² å€¼):** Over-forecast (å¯¦éš› < é æ¸¬) â†’ **åº«å­˜æˆæœ¬å¢åŠ **
                """)
            with c2:
                max_err = np.max(np.abs(res_vals))
                st.warning(f"""
                **ä¸ç¢ºå®šæ€§é‡åŒ– (Uncertainty Quantification):**
                * æœ€å¤§åå·® (Max Residual): **{max_err:.2f}**
                * å»ºè­°å®‰å…¨åº«å­˜ç·©è¡ (Safety Stock Buffer): **{max_err:.2f}**
                """)

        # --- Tab 3: ä¸­è‹±å°ç…§ ---
                # --- Tab 3: ä¸­è‹±å°ç…§èˆ‡ç ”ç©¶çµè«– ---
                with tab3:
                    # 1. è‡ªå‹•åŒ–å­¸è¡“çµè«– (Automated Academic Conclusion)
                    st.subheader("ğŸ“ ç ”ç©¶ç¸½çµ (Research Summary)")

                    # æ‰¾å‡ºè¡¨ç¾æœ€å¥½çš„æ¨¡å‹ (MAPE æœ€å°è€…)
                    best_model_name = min(["SMA", "Random Forest", "Linear Regression", "ARIMA"],
                                          key=lambda m: mean_absolute_percentage_error(y_true, results[m]))

                    # ç²å–è©²æ¨¡å‹çš„æœ€å¤§èª¤å·® (ä¸ç¢ºå®šæ€§é‡åŒ–)
                    current_res = y_true - results[selected_model]
                    max_err_val = np.max(np.abs(current_res))

                    st.markdown(f"""
                    **ã€ä¸­æ–‡ç¸½çµã€‘**
                    æœ¬ç ”ç©¶é‡å° **{dataset_option}** é€²è¡Œäº†å¤šæ¨¡å‹é©—è­‰ã€‚å¯¦é©—çµæœé¡¯ç¤ºï¼Œåœ¨æ­¤æ¡ˆä¾‹ä¸­ **{best_model_name}** è¡¨ç¾æœ€ç‚ºå„ªç•°ã€‚
                    é€éæ­¤æ¨¡å‹åˆ†æé æ¸¬èª¤å·®ï¼Œæˆ‘å€‘ç™¼ç¾ä¾›æ‡‰éˆä¸­çš„ã€Œä¸ç¢ºå®šæ€§ã€æœ€å¤§å€¼ç‚º **{max_err_val:.2f}**ã€‚
                    æ ¹æ“šæµç¨‹åœ–ä¸­çš„ã€Œåé¥‹å¾ªç’° (Feedback Loops)ã€ï¼Œä¼æ¥­æ‡‰ä»¥æ­¤æ•¸å€¼ä½œç‚ºå®‰å…¨åº«å­˜çš„ç·©è¡åŸºæº–ï¼Œä»¥é”æˆåº«å­˜å„ªåŒ–ä¸¦é™ä½æ–·è²¨é¢¨éšªã€‚

                    **ã€English Summaryã€‘**
                    This study conducted a multi-model validation for **{dataset_option}**. The results indicate that **{best_model_name}** is the best performer in this case. 
                    By analyzing the forecast errors, we quantified the maximum "Uncertainty" in the supply chain as **{max_err_val:.2f}**. 
                    Following the "Feedback Loops" in our flowchart, enterprises should use this value as the buffer for Safety Stock to achieve inventory optimization and mitigate stockout risks.
                    """)

                    st.markdown("---")

                    # 2. å°ˆæ¥­è¡“èªå°ç…§è¡¨ (Bilingual Glossary)
                    st.subheader("ğŸ“– å°ˆæ¥­è¡“èªå°ç…§ (Bilingual Glossary)")

                    # å»ºç«‹å°ç…§è¡¨è³‡æ–™
                    glossary_data = {
                        "é …ç›® (Item)": [
                            "Actual Demand", "Residuals", "Disruption",
                            "Visibility", "Adaptability", "Safety Stock"
                        ],
                        "ä¸­æ–‡è§£é‡‹ (Chinese Explanation)": [
                            "å¯¦éš›éœ€æ±‚ï¼šå¸‚å ´çœŸå¯¦ç™¼ç”Ÿçš„éŠ·å”®æ•¸æ“šã€‚",
                            "æ®˜å·®ï¼šå¯¦éš›å€¼èˆ‡é æ¸¬å€¼çš„å·®è·ï¼Œç”¨ä¾†é‡åŒ–ã€Œä¸ç¢ºå®šæ€§ã€ã€‚",
                            "æ“¾å‹•ï¼šä¾›æ‡‰éˆä¸­çªç™¼çš„æ„å¤–äº‹ä»¶ï¼ˆå¦‚ç–«æƒ…ã€æ–·è²¨ï¼‰ã€‚",
                            "å¯è¦–åŒ–ï¼šé€éæ•¸æ“šåœ–è¡¨æ¸…æ™°æŒæ¡éœ€æ±‚è¶¨å‹¢ã€‚",
                            "é©æ‡‰æ€§ï¼šç³»çµ±æ ¹æ“šåé¥‹è‡ªå‹•èª¿æ•´æ±ºç­–çš„èƒ½åŠ›ã€‚",
                            "å®‰å…¨åº«å­˜ï¼šç‚ºäº†æ‡‰å°é æ¸¬ä¸æº–ç¢ºè€Œé¡å¤–æº–å‚™çš„åº«å­˜ç·©è¡ã€‚"
                        ],
                        "English Definition": [
                            "Real-world sales data observed in the market.",
                            "The gap between actual and forecast; used to quantify Uncertainty.",
                            "Unexpected events in the supply chain (e.g., pandemics, shortages).",
                            "Clear transparency of demand trends through data visualization.",
                            "The system's ability to adjust decisions based on feedback.",
                            "The inventory buffer kept to protect against forecast errors."
                        ]
                    }
                    st.table(pd.DataFrame(glossary_data))

else:
    st.error("è«‹ç¢ºä¿ meat_consumption_worldwide.csv æª”æ¡ˆå­˜åœ¨ã€‚")
